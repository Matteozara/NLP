{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Matteo_zaramella.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Homowerk 1 NLP**\n",
        "\n",
        "This is the Colab Notebook for the first homework of NLP. I write and use it to create and train my model.\n",
        "There you can find the final version (the ordered one).\n",
        "\n",
        "To create this notebook I take some code and ispiration from the notebook presented at lessons and from 'https://scikit-learn.org/' for the confusion matrix."
      ],
      "metadata": {
        "id": "Qb5uoGbByoOb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "VAzxnophyklG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XLONlMkRWpiR"
      },
      "outputs": [],
      "source": [
        "#import\n",
        "import re\n",
        "from collections import Counter\n",
        "from typing import *\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "import plotly.express as px\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#fix random seed\n",
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "002sca1WWxy1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#path_train = '/content/drive/My Drive/results/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BTvlhpiWz3k",
        "outputId": "fa3ca6ac-4b32-4b99-b9ee-29bf2f189ced"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Dataset\n",
        "\n",
        "In this section (the larger one), I define all the function to model and change the format and the dimension of the dataset.\n",
        "I create also the vocabulary and the weights to asign on each classes."
      ],
      "metadata": {
        "id": "ltIoReiXVQPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function to load dataset from the path given\n",
        "def load_dataset(path):\n",
        "  data = pd.read_table(path, error_bad_lines=False, warn_bad_lines=False)\n",
        "  # Remove the rows of the file that contain a \"NaN\" value.\n",
        "  data.dropna() #if i remove this parameter it takes all the word (doesn't divide by phrases)\n",
        "  return data"
      ],
      "metadata": {
        "id": "WbfKgS-gW5Qh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load glove embeddings\n",
        "import gensim.downloader as gen\n",
        "\n",
        "wv = gen.load('glove-wiki-gigaword-300')"
      ],
      "metadata": {
        "id": "yUg5UKuxj-c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4649570d-a68d-43aa-95cf-9ee1caf033b8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create vocabulary list\n",
        "words = wv.vocab"
      ],
      "metadata": {
        "id": "bScg-dBHkCst"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create a dictionary from glove: word to index and index to word\n",
        "\n",
        "def default(words):\n",
        "  dictionary_indices = {}\n",
        "  dictionary_words = []\n",
        "  #embeds = torch.randn(len(words), 100)\n",
        "  dictionary_indices['pad'] = 0\n",
        "  dictionary_words.append('pad')\n",
        "  #embeds[0] = torch.zeros(100)\n",
        "  dictionary_indices['unk'] = 1\n",
        "  dictionary_words.append('unk')\n",
        "  #embeds[1] = torch.rand(100)\n",
        "\n",
        "  return dictionary_indices, dictionary_words#, embeds\n",
        "\n",
        "\n",
        "def create_dictionary(words, wv):\n",
        "  dictionary_indice, dictionary_words = default(words)\n",
        "  for i in words:\n",
        "    #print(i)\n",
        "    if i != 'unk' and i != 'pad':\n",
        "      dictionary_indice[i] = len(dictionary_words)\n",
        "      #elem = wv[i]\n",
        "      #elem = torch.from_numpy(elem)\n",
        "      #embeds[len(dictionary_words)] = elem\n",
        "      dictionary_words.append(i)\n",
        "\n",
        "  return dictionary_indice, dictionary_words"
      ],
      "metadata": {
        "id": "5HuaCg3WkDwT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create 'word_to_indices' and 'indices_to_word' with all the words on glove-wiki\n",
        "word_to_indices, indices_to_word = create_dictionary(words, wv)"
      ],
      "metadata": {
        "id": "Gc826LKgmBka"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#transform data frame in list\n",
        "def create_list(train_data):\n",
        "  c = 0\n",
        "  n_word = 0\n",
        "  max = 0\n",
        "  lista_train= {}\n",
        "  lista = []\n",
        "  for i in range(0, len(train_data)):\n",
        "    if(train_data['#'][i] == '#'):\n",
        "      lista_train[c] = lista\n",
        "      if max < n_word:\n",
        "        max = n_word\n",
        "      lista = []\n",
        "      n_word = 0\n",
        "      c += 1\n",
        "    else:\n",
        "      lista.append([train_data['#'][i], train_data['id'][i]])\n",
        "      n_word += 1\n",
        "\n",
        "  return lista_train, max"
      ],
      "metadata": {
        "id": "BaA0DyBbqSNt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#transform data so each phrase has the same length\n",
        "def all_same_length(train, max):\n",
        "  for i in train:\n",
        "    diff = max - len(train[i]) \n",
        "    if diff > 0:\n",
        "      for x in range(0, diff):\n",
        "        train[i].append(['pad', 'PAD'])\n",
        "\n",
        "  return train"
      ],
      "metadata": {
        "id": "F8KMY6Nlg2i0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Word into indices using vocabolary just create\n",
        "def dataset_to_indices(lista_train, w):\n",
        "  word_indices = {}\n",
        "  c = 0\n",
        "\n",
        "  for i in lista_train:\n",
        "    indices = []\n",
        "    for j in lista_train[i]:\n",
        "      #print(j[1])\n",
        "      try:\n",
        "        indices.append(w[j[0]])\n",
        "      except:\n",
        "        indices.append(1) #if the word isn't on dictionary\n",
        "    word_indices[c] = indices\n",
        "    c += 1\n",
        "  return word_indices"
      ],
      "metadata": {
        "id": "L-uAQKhxhRat"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Increase dicrionary with unknown train words and encode phrases\n",
        "def training_to_indices(lista_train, unigram, biagram, triagram, quadrigram, word_to_indices, indices_to_word):\n",
        "  word_indices = {}\n",
        "  c = 0\n",
        "\n",
        "  for i in lista_train:\n",
        "    indices = []\n",
        "    for j in lista_train[i]:\n",
        "      #print(j[1])\n",
        "      try:\n",
        "        indices.append(word_to_indices[j[0]])\n",
        "      except:\n",
        "        word_to_indices[j[0]] = len(indices_to_word)\n",
        "        indices.append(len(indices_to_word)) #if the word isn't on dictionary\n",
        "        indices_to_word.append(j[0])\n",
        "    word_indices[c] = indices\n",
        "    c += 1\n",
        "\n",
        "  for i in unigram:\n",
        "    #print('i: ', i)\n",
        "    indices = []\n",
        "    for j in unigram[i]:\n",
        "      #print('j: ',j)\n",
        "      indices.append(word_to_indices[j[0]])\n",
        "    word_indices[c] = indices\n",
        "    c += 1\n",
        "\n",
        "  for i in biagram:\n",
        "    #print('i: ', i)\n",
        "    indices = []\n",
        "    for j in biagram[i]:\n",
        "      #print('j: ',j)\n",
        "      indices.append(word_to_indices[j[0]])\n",
        "    word_indices[c] = indices\n",
        "    c += 1\n",
        "\n",
        "  for i in triagram:\n",
        "    indices = []\n",
        "    for j in triagram[i]:\n",
        "      indices.append(word_to_indices[j[0]])\n",
        "    word_indices[c] = indices\n",
        "    c += 1\n",
        "\n",
        "  for i in quadrigram:\n",
        "    indices = []\n",
        "    for j in quadrigram[i]:\n",
        "      indices.append(word_to_indices[j[0]])\n",
        "    word_indices[c] = indices\n",
        "    c += 1\n",
        "\n",
        "\n",
        "  return word_indices, word_to_indices, indices_to_word"
      ],
      "metadata": {
        "id": "4YyyHCAZpfjj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create 'indices_to_label' and 'label_to_indices'\n",
        "label_to_indices = {'PAD' : 0, 'O' : 1, 'B-PER' : 2,'I-PER' : 3,'B-LOC' : 4,'I-LOC' : 5,'B-GRP' : 6,'I-GRP' : 7,'B-CORP' : 8,'I-CORP' : 9,'B-PROD' : 10,'I-PROD' : 11,'B-CW' : 12,'I-CW' : 13}\n",
        "indices_to_label = ['PAD', 'O', 'B-PER' ,'I-PER' ,'B-LOC' ,'I-LOC' ,'B-GRP' ,'I-GRP' ,'B-CORP' ,'I-CORP' ,'B-PROD' ,'I-PROD' ,'B-CW' ,'I-CW']"
      ],
      "metadata": {
        "id": "6JWsJd5kqlVc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#encode all labels of training set\n",
        "def label_index_train(lista_train, unigram, biagram, triagram, quadrigram, l):\n",
        "  word_label = {}\n",
        "  a = {'PAD' : 0, 'O' : 0, 'B-PER' : 0,'I-PER' : 0,'B-LOC' : 0,'I-LOC' : 0,'B-GRP' : 0,'I-GRP' : 0,'B-CORP' : 0,'I-CORP' : 0,'B-PROD' : 0,'I-PROD' : 0,'B-CW' : 0,'I-CW' : 0}\n",
        "  c = 0\n",
        "\n",
        "  for i in lista_train:\n",
        "    label = []\n",
        "    for j in lista_train[i]:\n",
        "      #print(j[1])\n",
        "      label.append(l[j[1]])\n",
        "      a[j[1]] += 1\n",
        "    word_label[c] = label\n",
        "    c += 1\n",
        "\n",
        "  for i in unigram:\n",
        "    label = []\n",
        "    for j in unigram[i]:\n",
        "      label.append(l[j[1]])\n",
        "      a[j[1]] += 1\n",
        "    word_label[c] = label\n",
        "    c += 1\n",
        "\n",
        "  for i in biagram:\n",
        "    label = []\n",
        "    for j in biagram[i]:\n",
        "      label.append(l[j[1]])\n",
        "      a[j[1]] += 1\n",
        "    word_label[c] = label\n",
        "    c += 1\n",
        "\n",
        "  for i in triagram:\n",
        "    label = []\n",
        "    for j in triagram[i]:\n",
        "      label.append(l[j[1]])\n",
        "      a[j[1]] += 1\n",
        "    word_label[c] = label\n",
        "    c += 1\n",
        "\n",
        "  for i in quadrigram:\n",
        "    label = []\n",
        "    for j in quadrigram[i]:\n",
        "      label.append(l[j[1]])\n",
        "      a[j[1]] += 1\n",
        "    word_label[c] = label\n",
        "    c += 1\n",
        "\n",
        "    \n",
        "\n",
        "  return word_label, a"
      ],
      "metadata": {
        "id": "IPnW01RXJAMR"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#label to index (for validation)\n",
        "def label_index(lista_train, l):\n",
        "  word_label = {}\n",
        "  a = {'PAD' : 0, 'O' : 0, 'B-PER' : 0,'I-PER' : 0,'B-LOC' : 0,'I-LOC' : 0,'B-GRP' : 0,'I-GRP' : 0,'B-CORP' : 0,'I-CORP' : 0,'B-PROD' : 0,'I-PROD' : 0,'B-CW' : 0,'I-CW' : 0}\n",
        "  c = 0\n",
        "\n",
        "  for i in lista_train:\n",
        "    label = []\n",
        "    for j in lista_train[i]:\n",
        "      #print(j[1])\n",
        "      label.append(l[j[1]])\n",
        "      a[j[1]] += 1\n",
        "    word_label[c] = label\n",
        "    c += 1\n",
        "  return word_label, a"
      ],
      "metadata": {
        "id": "YBJAERJQqgS5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create training set (put together all the upper functions)\n",
        "def dataset_creator_lstm_training(data, word_to_indices, indices_to_word, label_to_indices):\n",
        "  lista, max = create_list(data)\n",
        "\n",
        "  same_length = all_same_length(lista, max)\n",
        "  word_indices, word_to_indices, indices_to_word = training_to_indices(same_length, [], [], [], [], word_to_indices, indices_to_word)\n",
        "  label_ind, count_classes = label_index_train(same_length, [], [], [], [], label_to_indices)\n",
        "  data = {}\n",
        "  for i in range(0, len(word_indices)):\n",
        "    data[i]={'inputs': torch.LongTensor(word_indices[i]), 'outputs': torch.LongTensor(label_ind[i])}\n",
        "\n",
        "  return data, count_classes, word_to_indices, indices_to_word"
      ],
      "metadata": {
        "id": "k01o-q-xs2t_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create validation (and test) set (put together all the upper functions)\n",
        "def dataset_creator_lstm(data, word_to_indices, label_to_indices):\n",
        "  lista, max = create_list(data)\n",
        "  same_length = all_same_length(lista, max)\n",
        "  word_indices = dataset_to_indices(same_length, word_to_indices)\n",
        "  label_ind, count_classes = label_index(same_length, label_to_indices)\n",
        "\n",
        "  data = {}\n",
        "  for i in range(0, len(word_indices)):\n",
        "    data[i]={'inputs': torch.LongTensor(word_indices[i]), 'outputs': torch.LongTensor(label_ind[i])}\n",
        "\n",
        "  return data, count_classes"
      ],
      "metadata": {
        "id": "ewnU6n1ngOKm"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function that create the weights for cross entropy function\n",
        "def param_classes(count_classes, params):\n",
        "  sum = 0\n",
        "  vec = [0]\n",
        "  for i in count_classes:\n",
        "    if i is not 'PAD':\n",
        "      sum += count_classes[i]\n",
        "  for i in count_classes:\n",
        "    if i is not 'PAD':\n",
        "      a = count_classes[i]/sum\n",
        "      #vec.append(1.0/a)\n",
        "      vec.append(1-a)\n",
        "\n",
        "  #for i in range(2, len(vec)):\n",
        "  #  vec[i] *= 1.5\n",
        "  \n",
        "  params.class_vector = torch.tensor(vec)"
      ],
      "metadata": {
        "id": "JXEKM-2BrQ_g"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create dataloader and full vocabulary\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "#device = \"cuda\"\n",
        "training_data_cnn = load_dataset('/content/drive/MyDrive/data (1)/train.tsv')\n",
        "dev_data_cnn = load_dataset('/content/drive/MyDrive/data (1)/dev.tsv')\n",
        "trainingset_lstm, count_classes_train, word_to_indices, indices_to_word = dataset_creator_lstm_training(training_data_cnn, word_to_indices, indices_to_word, label_to_indices)\n",
        "devset_lstm, count_classes_devset = dataset_creator_lstm(dev_data_cnn, word_to_indices, label_to_indices)\n",
        "\n",
        "train_dataset_lstm = DataLoader(trainingset_lstm, batch_size=128, shuffle=True)\n",
        "valid_dataset_lstm = DataLoader(devset_lstm, batch_size=128, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScCjQcHBfn5s",
        "outputId": "eec41c94-1172-4b9d-9a3d-aa79a778745b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: FutureWarning: The warn_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: FutureWarning: The warn_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create embed's matrix\n",
        "def create_embed_weights(word_to_indices, wv):\n",
        "  embeds = torch.randn(len(word_to_indices), 300)\n",
        "  embeds[0] = torch.zeros(300)\n",
        "  embeds[1] = torch.rand(300)\n",
        "  for i in word_to_indices:\n",
        "    if i is not 'pad'and i is not 'unk':\n",
        "      try:\n",
        "        elem = wv[i]\n",
        "        elem = torch.from_numpy(elem)\n",
        "        embeds[word_to_indices[i]] = elem\n",
        "      except:\n",
        "        embeds[word_to_indices[i]] = torch.rand(300)\n",
        "  return embeds"
      ],
      "metadata": {
        "id": "B7Jl9mr5usp6"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create embeds\n",
        "embeds_pretrained = create_embed_weights(word_to_indices, wv)"
      ],
      "metadata": {
        "id": "PV7_jvQzvwu3"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameters\n",
        "\n",
        "Set the hyperparameters for the model. \n",
        "I have created an external class (like in  Notebook of Post-Tagging) because is easyer to modify."
      ],
      "metadata": {
        "id": "KZgexIKNyR-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#class params to set all the hyperparameters\n",
        "class HParams_bilstm():\n",
        "    vocab_size = len(indices_to_word)\n",
        "    hidden_dim = 128\n",
        "    embedding_dim = 300\n",
        "    num_classes = len(indices_to_label)\n",
        "    bidirectional = True\n",
        "    num_layers = 2\n",
        "    dropout = 0.005\n",
        "    embeddings = embeds_pretrained\n",
        "    class_vector = []\n",
        "    \n",
        "params = HParams_bilstm()"
      ],
      "metadata": {
        "id": "_W9Si7_BrxW6"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set weights for cross entropy\n",
        "param_classes(count_classes_train, params)"
      ],
      "metadata": {
        "id": "sjA76UzttsHx"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = '/content/drive/MyDrive/data (1)/emebds11.pt'\n",
        "torch.save(embeds_pretrained, PATH)"
      ],
      "metadata": {
        "id": "bpTIBJujRV_Z"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model\n",
        "\n",
        "I have create a simple model:\n"
      ],
      "metadata": {
        "id": "HF5FXvc7x1fu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create model\n",
        "class Embed_classifier(nn.Module):\n",
        "\n",
        "    def __init__(self, hparams):\n",
        "        super(Embed_classifier, self).__init__()\n",
        "        self.word_embedding = nn.Embedding(hparams.vocab_size, hparams.embedding_dim)\n",
        "        self.word_embedding.weight.data.copy_(hparams.embeddings)\n",
        "\n",
        "        self.lstm = nn.LSTM(hparams.embedding_dim, hparams.hidden_dim, \n",
        "                            bidirectional=hparams.bidirectional,\n",
        "                            num_layers=hparams.num_layers,\n",
        "                            batch_first = True,\n",
        "                            dropout = hparams.dropout if hparams.num_layers > 1 else 0)\n",
        "        \n",
        "        lstm_output_dim = hparams.hidden_dim * 2\n",
        "    \n",
        "        self.fc1 = nn.Linear(lstm_output_dim, 64)\n",
        "\n",
        "        self.dropout = nn.Dropout(hparams.dropout)\n",
        "\n",
        "        self.classifier = nn.Linear(64, hparams.num_classes)\n",
        "\n",
        "    \n",
        "    def forward(self, x):\n",
        "        embeddings = self.word_embedding(x)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        x, (h, c) = self.lstm(embeddings)\n",
        "        x = self.dropout(x)\n",
        "        o = self.fc1(x)\n",
        "        o = self.dropout(o)\n",
        "        \n",
        "\n",
        "        output = self.classifier(o)\n",
        "        return output"
      ],
      "metadata": {
        "id": "sLA_oJ4SWPmL"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer():\n",
        "    \"\"\"Utility class to train and evaluate a model.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        loss_function,\n",
        "        optimizer,\n",
        "        label_vocab: indices_to_label,\n",
        "        log_steps:int=10_000,\n",
        "        log_level:int=2):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            model: the model we want to train.\n",
        "            loss_function: the loss_function to minimize.\n",
        "            optimizer: the optimizer used to minimize the loss_function.\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.loss_function = loss_function\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "        self.label_vocab = label_vocab\n",
        "        self.log_steps = log_steps\n",
        "        self.log_level = log_level\n",
        "        self.label_vocab = label_vocab\n",
        "\n",
        "    def train(self, train_dataset:Dataset, \n",
        "              valid_dataset:Dataset, \n",
        "              epochs:int=1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            train_dataset: a Dataset or DatasetLoader instance containing\n",
        "                the training instances.\n",
        "            valid_dataset: a Dataset or DatasetLoader instance used to evaluate\n",
        "                learning progress.\n",
        "            epochs: the number of times to iterate over train_dataset.\n",
        "\n",
        "        Returns:\n",
        "            avg_train_loss: the average training loss on train_dataset over\n",
        "                epochs.\n",
        "        \"\"\"\n",
        "        assert epochs > 1 and isinstance(epochs, int)\n",
        "        if self.log_level > 0:\n",
        "            print('Training ...')\n",
        "        train_loss = 0.0\n",
        "        for epoch in range(epochs):\n",
        "            if self.log_level > 0:\n",
        "                print(' Epoch {:03d}'.format(epoch + 1))\n",
        "\n",
        "            epoch_loss = 0.0\n",
        "            self.model.train()\n",
        "\n",
        "            # for each batch \n",
        "            for step, sample in enumerate(train_dataset):\n",
        "                inputs = sample['inputs']\n",
        "                labels = sample['outputs']\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                predictions = self.model(inputs)\n",
        "                predictions = predictions.view(-1, predictions.shape[-1])\n",
        "                labels = labels.view(-1)\n",
        "                # labels  [[1,2,3], [18, 12, 3]] after the view(-1) [1,2,3, 18, 12, 3]\n",
        "                \n",
        "                sample_loss = self.loss_function(predictions, labels)\n",
        "                sample_loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                epoch_loss += sample_loss.tolist()\n",
        "\n",
        "                if self.log_level > 1 and step % self.log_steps == self.log_steps - 1:\n",
        "                    print('\\t[E: {:2d} @ step {}] current avg loss = {:0.4f}'.format(epoch, step, epoch_loss / (step + 1)))\n",
        "            \n",
        "            avg_epoch_loss = epoch_loss / len(train_dataset)\n",
        "            train_loss += avg_epoch_loss\n",
        "            if self.log_level > 0:\n",
        "                print('\\t[E: {:2d}] train loss = {:0.4f}'.format(epoch, avg_epoch_loss))\n",
        "\n",
        "            valid_loss = self.evaluate(valid_dataset)\n",
        "            \n",
        "            if self.log_level > 0:\n",
        "                print('  [E: {:2d}] valid loss = {:0.4f}'.format(epoch, valid_loss))\n",
        "\n",
        "        if self.log_level > 0:\n",
        "            print('... Done!')\n",
        "        \n",
        "        avg_epoch_loss = train_loss / epochs\n",
        "        return avg_epoch_loss\n",
        "    \n",
        "\n",
        "    def evaluate(self, valid_dataset):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            valid_dataset: the dataset to use to evaluate the model.\n",
        "\n",
        "        Returns:\n",
        "            avg_valid_loss: the average validation loss over valid_dataset.\n",
        "        \"\"\"\n",
        "        valid_loss = 0.0\n",
        "        # set dropout to 0!! Needed when we are in inference mode.\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for sample in valid_dataset:\n",
        "                inputs = sample['inputs']\n",
        "                labels = sample['outputs']\n",
        "\n",
        "                predictions = self.model(inputs)\n",
        "                pred = predictions.view(-1, predictions.shape[-1])\n",
        "                lab = labels.view(-1)\n",
        "                #print('LABRLS: ', labels.tolist())\n",
        "                top_label_scores, top_label_indices = torch.max(pred, -1)\n",
        "                top_label_indices = top_label_indices.tolist()\n",
        "                #print('PREDICTION: ', top_label_indices)\n",
        "                sample_loss = self.loss_function(pred, lab)\n",
        "                valid_loss += sample_loss.tolist()\n",
        "                f1_score1 = f1_score(lab.data, top_label_indices, average=\"macro\")\n",
        "                print('f1: ', f1_score1)\n",
        "\n",
        "        \n",
        "        return valid_loss / len(valid_dataset)\n",
        "\n",
        "    def predict(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: a tensor of indices.\n",
        "        Returns: \n",
        "            A list containing the predicted POS tag for each token in the\n",
        "            input sentences.\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            logits = self.model(x)\n",
        "            predictions = torch.argmax(logits, -1)\n",
        "            return logits, predictions\n",
        "    \n",
        "    \n"
      ],
      "metadata": {
        "id": "eW3q188Lt2AK"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "q0mGa_mbx6Ox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bilstm_embed_classifier = Embed_classifier(params)#.cuda()"
      ],
      "metadata": {
        "id": "JGZwhQZRt9MU"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bilstm_trainer = Trainer(\n",
        "    model = bilstm_embed_classifier,\n",
        "    loss_function = nn.CrossEntropyLoss(weight=params.class_vector, ignore_index=label_to_indices['PAD']),\n",
        "    optimizer = torch.optim.Adam(bilstm_embed_classifier.parameters()), #lr=0.006\n",
        "    label_vocab=indices_to_label\n",
        ")"
      ],
      "metadata": {
        "id": "y9lnuzA3uA1o"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bilstm_trainer.train(train_dataset_lstm, valid_dataset_lstm, 6)\n",
        "PATH = '/content/drive/MyDrive/data (1)/model_prova1.pt'\n",
        "torch.save(bilstm_embed_classifier, PATH)\n",
        "#torch.save(bilstm_embed_classifier.state_dict(), PATH)"
      ],
      "metadata": {
        "id": "ziXLy55WuEtd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f69a75b-4d20-46eb-d60e-9b27bf857c75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training ...\n",
            " Epoch 001\n",
            "\t[E:  0] train loss = 1.2813\n",
            "f1:  0.49520657224123027\n",
            "f1:  0.44371812814134143\n",
            "f1:  0.4794639623316116\n",
            "f1:  0.43801838888985994\n",
            "f1:  0.4564337965646419\n",
            "f1:  0.4108731128444825\n",
            "  [E:  0] valid loss = 0.6842\n",
            " Epoch 002\n",
            "\t[E:  1] train loss = 0.5204\n",
            "f1:  0.5673852947792597\n",
            "f1:  0.5512076554276761\n",
            "f1:  0.5635470930286383\n",
            "f1:  0.5173189643583151\n",
            "f1:  0.5204331253917803\n",
            "f1:  0.5019157341979196\n",
            "  [E:  1] valid loss = 0.5359\n",
            " Epoch 003\n",
            "\t[E:  2] train loss = 0.3337\n",
            "f1:  0.5794384729961038\n",
            "f1:  0.5708326279844964\n",
            "f1:  0.5774498765444795\n",
            "f1:  0.5364975814291415\n",
            "f1:  0.5198896713940628\n",
            "f1:  0.5340827327331615\n",
            "  [E:  2] valid loss = 0.4963\n",
            " Epoch 004\n",
            "\t[E:  3] train loss = 0.2069\n",
            "f1:  0.6461555222959197\n",
            "f1:  0.5823369712466245\n",
            "f1:  0.6225941882739837\n",
            "f1:  0.5845505316059848\n",
            "f1:  0.5802762256849815\n",
            "f1:  0.5642486616984699\n",
            "  [E:  3] valid loss = 0.5315\n",
            " Epoch 005\n",
            "\t[E:  4] train loss = 0.1237\n",
            "f1:  0.6605955215552743\n",
            "f1:  0.6412823072371545\n",
            "f1:  0.6312340227431752\n",
            "f1:  0.595481595098924\n",
            "f1:  0.5943427063766743\n",
            "f1:  0.578247663660186\n",
            "  [E:  4] valid loss = 0.6013\n",
            " Epoch 006\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction\n",
        "\n",
        "Predict the Validation set and create the Confusion Matrix associated"
      ],
      "metadata": {
        "id": "QO4Ti18byE-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create validation (and test) set (put together all the upper functions)\n",
        "def dataset_prediction(data, word_to_indices, label_to_indices):\n",
        "  lista, _ = create_list(data)\n",
        "  word_indices = dataset_to_indices(lista, word_to_indices)\n",
        "  label_ind, count_classes = label_index(lista, label_to_indices)\n",
        "\n",
        "  data = {}\n",
        "  for i in range(0, len(word_indices)):\n",
        "    data[i]={'inputs': torch.LongTensor(word_indices[i]), 'outputs': torch.LongTensor(label_ind[i])}\n",
        "\n",
        "  return data, count_classes"
      ],
      "metadata": {
        "id": "7XbIJYnqmaPs"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create validation dataloader for prediction\n",
        "dev_data_cnn = load_dataset('/content/drive/MyDrive/data (1)/dev.tsv')\n",
        "devset_lstm, _ = dataset_creator_lstm(dev_data_cnn, word_to_indices, label_to_indices)\n",
        "valid_dataset_lstm = DataLoader(devset_lstm, batch_size=128, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6K6dU-al_JH",
        "outputId": "cbdea1ed-ef90-4125-ec56-56df17fab4f5"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: FutureWarning: The warn_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = []\n",
        "logit = []\n",
        "true_label = []\n",
        "result = []\n",
        "true_result = []\n",
        "for i in valid_dataset_lstm:\n",
        "  inputs = i['inputs']\n",
        "  labels = i['outputs']\n",
        "  logits, predict = bilstm_trainer.predict(inputs)\n",
        "  for i in range(0, predict.size(dim=0)):\n",
        "    pre = predict[i].numpy()\n",
        "    lab = labels[i].numpy()\n",
        "    pred.append(pre)\n",
        "    true_label.append(lab)\n",
        "  for i in pred:\n",
        "    for j in i:\n",
        "      \n",
        "      result.append(j)\n",
        "  for i in true_label:\n",
        "    for j in i:\n",
        "      true_result.append(j)"
      ],
      "metadata": {
        "id": "CaMPYxo7eQi7"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_matrix(result, true_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "a5HyIWijgJm2",
        "outputId": "71ce2855-dd67-4b78-c585-9cc486061ec0"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeQAAAHICAYAAACfwJm0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de5wddX3/8dd7EyCRgFwSBCHhIlA0QALZggZUwEuhpSC1FBALov4i/QEVKooIdTIghAoKXmglFlD8FWtV1Ki0KEqUQrQECJAEBIQIgSKEUq7hkuTz+2PmyOFwdrObnTnnzOz7yWMeu2cu3893dsl+5/Od73xHEYGZmZl1V1+3K2BmZmZukM3MzHqCG2QzM7Me4AbZzMysB7hBNjMz6wFukM3MzHrA2G5XwMzMbF1N1th4nmIf313Bmmsi4sBCCx0CN8hmZlZZzxO8lw0LLfMSnp5YaIFD5AbZzMwqS9Tn3mtdzsPMzKzSnCGbmVml9UnFFtilGaWdIZuZmfUAZ8hmZlZZdbqH7AbZzMwqra/gHutudVm7QbZRJU3TvwWOB7YHxgGnJElyUckxlwEkSbJdmXFGkzRN5wNvT5Kk6D/FZl3jBtlKkabpLsAJwP7AZGA8sAK4FbgK+H9JkrzQ4TodCXwhr8NFwAvArzpZB8ukaRrAL5Ik2a/bdbHqc5e12QDSNP00kJD9O1kAfB14BngdsB/wz8DfAP0drtrBja9Jkjzcwbjv6GCs0eIY4DXdroRZkdwgW6HSNP0UkAIPAocnSfLrNvscDHys03UDXg/Q4caYJEl+28l4o0GSJA90uw7WG4SKf+ypSxTRpbvXVjtpmm4H3J1/3DNJksWD7LtBa5d1mqZ/BZwITAPWB+4FrgQ+32bfZfm3U4HZwBFkGfiDwFeBzyZJEvm+s8ky9ldJkkR5ve8Hvp4kyQfa1HU+Lfcr0zQVWZb2EWAnYCPgMWApcFmSJN9qrWvrPeQ0TTcATgGOBt4ArAJuA76UJMm/tez7hzrm53se8E5gArAYmJ0kyY/anWM7jS5jsp/bHODP8rJuA05LkuT6NE03zGP9FbAl2e9jdpIk324p67XALOAgYGdgC+BJst6ROUmSLGja9wPA5QNVK0mS2S3nei5wNtmtj4nAAUmSzG/9naRpuj5wA1mvy6FJksxrqeMVwF8Dn06S5Oyh/pys922psXHMmAmFlnn+6idvjohO9+DVpuvdesNxwHrAdwdrjAHaNLDnAt8C3kjWCH+Z7ImGc4Fr8j+4rdYDrgHeC/w7WVf4eLLG6tNN+80ny9p/1wjXtKyrc4CvkTVU/wZ8HrgW2Bo4fG0H5+dzDVljOBa4GPgGWYP2rfzn0c62wH8B2+X7fwvYFfhBmqb7D/McNiFrxPYAvgl8l6xBuyZN02nAz4BDgR+RNY5T8rq9uaWcN5L9PNYAPyb7WfwUOAD4ZZqmzZP0L+Lln/vveOXvYn5LuW8Afp2f678Ac4Gn2p1IkiQvkl1cPAVcnqbp5Ma2NE2PI2uMf5bX06wnucvairRv/vVnwzkoTdO3AKeTZbd7JUnySL7+dOB7ZPd+TyVrnJu9niyje1eSJCvzY1KyLP2UNE3PTZLkpSRJ5gPz0zTdD9g2SZLZwz+1V/kI8BCwa5Ikz7Wcz1Ampv8Y8HayC4lDkiRZ1VT//wJOT9P0R0mS3Nhy3H5kWeofLibSNL0S+A/g48B1wziHacAlwP9NkmRNXtZPgSvycm4A9kuS5Pl82zeAXwKnAYc1lXMn8PokSVY0F56m6Tb5uVyY148kSRYBi9I0TYBla/ld7EuWYX9qKCeTJMl9aZr+H7KLlCvz3/fOZBd3jwLvb5yn1Yco4bGnLnGGbEXaKv+6fJjHfTD/+plGYwyQN1IfI8u8PjzAsX/baIzzYx4FfgC8FvijYdZjuF4CVreubG2YBvBBsqcd/67RGOfHPkrWRQvtz/l3wGda4l0DPADsNbRq/8FzwMdbGqkrybrONwU+2miM8zjXA8uA6S3xn2x3zkmSLAe+A+ySpumUYdYN4PcMsxcj7+q/hKwx/wey3ovxwF83/79l1oucIVsv2DP/+vPWDUmS3J2m6XJg+zRNX5skyZNNm59MkuTeNuU9mH/dtOB6NvsX4CRgaZqm/0Z2P3ZBS/3aStN0I2BH4KEkSe5qs0vj57BHm22LkiR51UUA2Tm/ZUg1f9ndSZI83bwiSZLVaZr+HtgwSZL72hzzELB368o0TfcBPprXYQuyMQDNtia7aBiO29bx0biTgZm8PHBwTpIkP1mHcqwi6pJZ1uU8rDf8d/5162Ee99qW4wcqd5OW9f87wP6NjHPMMOsxHKfkyzPAJ8m6nlekafqDNE13XMux63q+MPg5D/ff80AXD6vWsu0VF/Jpmh5G1pX9Z8DNZF3EZ5Nlt7/Id9tgmHUDWKeMNs/qf9xU34vXpRyrCIGkQpducYZsRfpPsoE87wAuHcZxjT/+WwLtHhHaqmW/ojW6bAf69/CqhjHPUi8CLkrTdAuyLtIjyQZ0TU3TdOog2V3z+bZT9vkW7WzgRaA/SZI7mzekaXoJ2b3ydbFOj4Ckabov2f30FWQjsy9L0/TAxqh7s17lDNmKdDnZfdX3pmn6psF2zB/5abg1/7pfm/12BLYB7k+SZKDscKSeyL9Obt2QpunGZAODBpQkyaNJklyVJMlfkXU3v4Fs5PNA+z9NduGxdZqmO7XZpTFa+pYh1L0X7AgsbdMY9/HyQL9WayihByNN083JRoy/RHZx+C/Au8kGolkNNV4uUeTSLW6QrTBJkiwje251feDHaZq2fY4vfwzm35tWXZZ/PTNN00lN+40BLiD7/3Q4Gfew5A3kXcA+zRcSefzPkw0Komn9Bvk9U1rWrwdsln98rnV7i8vI/pacn8dplDER+PumfapgGbBTmqavb6zIn9OeDQx0YfY4bS6ACnA52QXcKUmS3EE2I9y9wNlpms4sIZ5ZYdxlbYVKkuTcNE3Hkk3EcVOapjcCC3l56sy3kU2ksbDpmBvTNP0s8AlgcZqm3wGeJZtoYleyrvDzS676+WSN/g1pmn4beJ4sU12P7NGqaU37jgf+M03Te8numf6O7EUV7yJ7Jndea7bYxgVk53cocFuapleTTQV5ONmgqM8mSfKfBZ1b2S4EvgLcmqbpd8my033IGuMfAn/e5pifAUemafpDsp6Al4BfJknyy3WtRJqmJ+exvpskyVcgu9hK0/QIsklKvpmm6fQkSZ4YrByrHj/2ZDaAJEnOImtIv0w2gOk4snt6f0bWVfthWroykyQ5DTgKuIdsBqy/Jfv/80yy54xfLLnOl+X1ehg4lmx2qhvJGpbWrvJnybpA7yUbzftR4H1kk1L8DUOYGCQ/n3cBZ+SrTsrj3gO8L/95VEKSJJeQ/Y7/m+wcjiYb9b03A3e7f5Ssa3kvst/x2WRdzOskTdMZZI85/Y6Wx8WSJLmF7P+/KQw8S5hZ13nqTDMzq6zX942Nj6y3UaFlzn7xf7sydaa7rM3MrLKymbrq0WftLmszM7Me4AzZzMwqrS6ZZV3Ow8zMrNKcIZuZWWXV6W1PbpDNzKzS6tLVW5fzMDMzqzRnyGZmVml91KPP2hmymZlZD3CGbGZmleVBXWZmZj2iLl29dTkPMzOzSnOGbGZmlSXVp8vaGbKZmVkPcIZsZmaVVpfHnmrZIE+cuHlsN2VK6XF+d+vtpcfYdo/dS49hZlamZQ88wIoVj5fWataly7qWDfJ2U6aw8D/nlx7n+A23KT3GVzpwHmY2ekVE6TH++K37lx6jDmrZIJuZ2egg6jMYqi7nYWZmVmnOkM3MrNLqcg/ZGbKZmVkPcIZsZmaVJeTHnszMzHqBu6zNzMysMB1tkCWtlrRI0mJJ35b0mnz9WEmPSTqvZf/5kn4j6XZJd0n6sqRNOllnMzPrbSp46ZZOZ8grI2J6ROwKvAgcn69/F3A3cLik1p/H0RGxO7A78ALwg47V1szMrEO62WV9PbBj/v1RwBeAB4C3tNs5Il4EPgFMkTStIzU0M7OeJrJ7yEUu3dKVBlnSWOAg4A5J44B3Aj8EvknWOLcVEauB24BdOlFPMzPrfX35SOuilu6dR2eNl7QIWEiWDV8KHAxcFxErge8C75E0ZpAy2v60JM2StFDSwsdWPF50vc3MzErV6ceeVkbE9OYVko4C9pW0LF+1OXAA8NPWg/OGejfgztZtETEXmAvQv+ce5c+WbmZmXacudzMXqauPPUnaGHgrMCUitouI7YATaNNtLWk9YA7wYESU/95DMzOzDur2xCCHAT+PiBea1v0A+KykDfLP/yLpBWAD4Frg0A7X0czMelhdJtToaIMcERNaPn8d+HrLuv8BJuUf9+tMzczMrKpq0mNdmwsLMzOzSut2l7WZmdk6y55DrkeO7AzZzMysBzhDNjOzSqtHfuwM2czMrCc4QzYzs0qrS4bsBtnMzCrNDbIxvs89/lauiM7MAvvqt55WVyd+Zp36eXXq92+9wQ2ymZlVWl0uKJ3imZmZ9QBnyGZmVlnC95DNzMx6Ql26eutyHmZmZpXmDNnMzCqtJmO6nCGbmZn1AmfIZmZWaarJsK6ez5AlbSPpB5LukfRbSV+QtH6362VmZt2nEpZu6ekGWdnT3lcB34+InYCdgQnAOV2tmJmZWcF6ukEGDgCej4jLASJiNXAK8EFJr+lqzczMrCc4Q+6MqcDNzSsi4ingAWDHrtTIzMysBL3eIA+ZpFmSFkpa+NiKx7tdHTMz65A+FbsMhaQDJf1G0r2SPtlm+xRJ10m6VdLtkv50recx/FPvqKXAjOYVkjYGpgD3Nq+PiLkR0R8R/ZMmbt7BKpqZ2WgiaQxwMXAQ8CbgKElvatntTODfImIP4EjgH9dWbq83yD8DXiPpGPjDD+FzwNci4rmu1szMzHqACv9vCPYC7o2I+yLiReBfgUNb9glg4/z71wIPr63Qnm6QI3sZ6GHA4ZLuAe4Gngc+1dWKmZlZT+jSY09bAw82fV6er2s2G3i/pOXA1cBJayu05ycGiYgHgT/vdj3MzGzUmChpYdPnuRExd5hlHEXWm/s5SW8BviFp14hYM9ABPd8gm5mZDUilzGW9IiL6B9n+EDC56fM2+bpmHwIOBIiIBZLGAROBRwcqtKe7rM3MzHrQTcBOkrbPZ448EpjXss8DwDsAJL0RGAc8NlihzpDNzKzSOj2ZR0SsknQicA0wBrgsIpZIOgtYGBHzgI8BX5V0CtkArw/k46IG5AbZzMwqra8L82tFxNVkg7Wa13266fulwD7DKdNd1mZmZj3AGbKZmVVWt+efLpIzZDMzsx7gDHkEPv/ft5ce4ytbvKH0GAAfeeSejsTpBPWVf50ZawZ8lLBYJTzPUXeq089s4EdWC6O+MaXHKFtdfuVukM3MrNJq0h67y9rMzKwXOEM2M7NKG+ILIXqeM2QzM7Me4AzZzMwqS0BfPRJkZ8hmZma9wBmymZlVWk0SZDfIZmZWbXVpkN1lbWZm1gMKaZAlrZa0SNJtkm6RNHOA/WZLeijfd7GkQ9qsbyybSNpP0pP557skXVBEfc3MrD5U8H/dUlSX9cqImA4g6U+AOcDbB9j3woi4IH9h8/WStmhe37xjPgXe9RFxsKTxwK2SvhcRNxRUbzMzs55Qxj3kjYEn1rZTRNwpaRUwcSiFRsRKSYuArUdYPzMzqxHPZf1K4/PGchywFXDA2g6QtDewBngsX3WKpPfn3z8REfu37L8psBPwy4LqbGZmFSfqMxiqjC7rtwBXSNo1IqLNvo2G92ngiIiIvGv6VV3WubdKuo2sMb4oIh5pVwFJs4BZAFMmTx75GZmZmXVQ4RcWEbGArBt6kqRzGoO0mna5MCKmR8RbI+L6IRR5fURMA6YCH5I0fYC4cyOiPyL6J03cfOQnYmZmlaCCl24pvEGWtAswBng8Is7IG9+2jehwRMT9wHnAaSMty8zMrNcUfQ8ZsguMYyNi9TDLaL6HDPCeNvt8BThV0nYRsWwd6mlmZjWjmozqKqRBjogxQ9xv9iDr221bBsxv2m8lHmVtZmZN6tEc12dwmpmZWaV5LmszM6usbg/EKpIzZDMzsx7gDNnMzKpLqs2gLmfIZmZmPcAZspmZVVpfPRJkN8hmZlZtqkmL7AZ5JMauV3qIjzxyT+kxAL6wxY6lxzh5xX2lxwCINWtKj6G+ztzt6cS5APV5XU7NqG9IUzyMSKxeVXoMaPdaA2vlBtnMzCpL1Od60oO6zMzMeoAzZDMzqy7VJ0N2g2xmZpXm55DNzMysMM6Qzcys0mqSIDtDNjMz6wXOkM3MrNJ8D3kQkp4ZYP1sSQ9JWiRpsaRD2qxvLJtI2k/Sk/nnuyRdUEZ9zcysmhrPIRe5dEs3uqwvjIjpwOHAZZL6mtc3Lf+br78+338P4GBJ+3ShzmZmZqXqWpd1RNwpaRUwcYj7r5S0CNi63JqZmVllCPrcZT0ykvYG1gCP5atOaequvq7N/psCOwG/7GA1zczMOqIbGfIpkt4PPA0cERGR35C/MCLa3SN+q6TbyBrjiyLikXaFSpoFzAKYMnlyOTU3M7OeU5MEudwMWdI5jay3aXXjXvFbI+L6IRRzfURMA6YCH5I0vd1OETE3Ivojon/SxM2LqL6ZmVnHlNogR8QZjUFaBZR1P3AecNrIa2ZmZvUgpGKXbumliUGa7yEvkrRdm32+ArxtgG1mZjbKCFBfsUu3lHIPOSImDLB+9iDr221bBsxv2m8lHmVtZmY15Jm6zMysuuSZuszMzKxAzpDNzKzSapIgu0E2M7Nqc5e1mZmZFcYZspmZVVpNEmRnyGZmZr3AGfJIxJpu16AwJ6+4r/QYX9niDaXHAPjII/eUHiPWrC49RhYoOhOnRtfm0YGfWafuWcaa8v/GaEwnmoHyfl6iPm97coNsZmbVJXdZm5mZWYGcIZuZWaX5sSczMzMrjDNkMzOrtJokyM6QzczMeoEzZDMzqyxRnwzZDbKZmVWXhPrq0SK7y9rMzKwHFNIgS1otaZGk2yTdImnmAPvNlnRqm/XvkXS7pDsl3SHpPS3bT5V0Vx7jJknHFFFvMzOrPqnYpVuK6rJeGRHTAST9CTAHePtQDpQ0DbgAeFdE3C9pe+Cnku6LiNslHQ+8C9grIp6StDFwWEH1NjMz6wll3EPeGHhiGPufCpwbEfcD5I3yHODjwF8DnwL2i4in8u1PAV8vtspmZlZVnsv6lcZLWgSMA7YCDhjGsVPJMuRmC4ET8mx4o4go/80HZmZWOXUaZV3UoK6VETE9InYBDgSuUIfnMpM0S9JCSQsfW/F4J0ObmZmNWOGjrCNiATARmCTpnHwg1qJBDlkKzGhZNwNYkndPPyNphyHEnRsR/RHRP2ni5utcfzMzqxZJhS7dUniDLGkXYAzweESckWfO0wc55ALgdEnb5cdvR3bf+HP59jnAxXn3NZImeJS1mZnVTdH3kCHr0j82IgZ6g/uZkk5ufIiIbSSdBvxQ0nrAS8AnIqJR3j8BE4CbJL2Ub/9ca6FmZjYK1eh9yIU0yBExZoj7zQZmt1l/FXDVAMcE8Nl8MTMzewW/ftHMzMwK47mszcys0mqSIDtDNjMz6wXOkM3MrLKyiUHqkSI7QzYzM+sBzpDNzKy6BKpJalmT0zAzs9Gp2Fm6htr9LelASb+RdK+kTw6wz19JWippiaQr11amM+SReOG50kNo/EalxwCIF58vPcZHli8uPQbA9TvsVnqMty1bUnoMgHhmOC9OG4EJm3YmTgfU5X4igPqcM/UiSWOAi8leDbycbOKqeRGxtGmfnYDTgX0i4glJW6ytXDfIZmZWbX0dvwjbC7i38SZCSf8KHEr2boaG/wNcHBFPAETEo2sr1JdfZmZmw7M18GDT5+X5umY7AztLukHSryQduLZCnSGbmVm1FX+bYqKkhU2f50bE3GGWMRbYCdgP2Ab4paTdIuJ/BzvAzMysmlTKuIEVEdE/yPaHgMlNn7fJ1zVbDvw6Il4C7pd0N1kDfdNAhbrL2szMbHhuAnaStL2k9YEjgXkt+3yfLDtG0kSyLuz7BivUGbKZmVVbhwd1RcQqSScC1wBjgMsiYomks4CFETEv3/ZuSUuB1cDHI+Lxwcp1g2xmZjZMEXE1cHXLuk83fR/A3+XLkLhBNjOzClNtXvdUyj1kSc8MsH62pFPbrH+PpNsl3SnpDknvadl+qqS7JC2SdJOkY8qot5mZVYsE6lOhS7d0PUOWNA24AHhXRNwvaXvgp5Lui4jbJR1PNhvKXhHxlKSNgcO6WWczM7Oi9cIo61OBcyPifoD86xzg4/n2TwF/ExFP5dufioivd6WmZmbWe6Rily7phQZ5KnBzy7qFwNQ8G96oMT2ZmZlZXXW9y7ookmYBswCmTJ68lr3NzKwuunnft0ilZsiSzskHYi0aZLelwIyWdTOAJXk39TOSdlhbrIiYGxH9EdE/aeLmI6i1mZlVirus1y4izoiI6RExfZDdLgBOl7QdQP71U8Dn8u1zgIvz7mskTfAoazMzq5tudFmfKenkxoeI2EbSacAPJa0HvAR8IiIaWfU/ARPI3jf5Ur79c62FmpnZKCR14/WLpSilQY6ICQOsnw3MbrP+KuCqAY4J4LP5YmZmVku1GdRlZmajUwlve+qKXnjsyczMbNRzhmxmZtXme8hmZmZdJvxyCTMzMyuOM2QzM6s01SS1rMlpmJmZVZsz5BGIF1aWH+SlF8qPAWj9caXHiDWrS48B8LZlS0qPce22byw9BsA7li3tSJxYs6b0GOrz9f9wRQf+/Wu9DUqPUbqa3EN2g2xmZtUl+eUSZmZmVhxnyGZmVm016bJ2hmxmZtYDnCGbmVm11eQeshtkMzOrLMkvlzAzM7MCOUM2M7Nqq0mXtTNkMzOzHjDiBlnSakmLJN0m6RZJMwfZdydJP5L0W0k3S7pO0tvybR+Q9Fhe1l2STmk6brakh/JtiyUdMtJ6m5lZHahxI7m4pUuKyJBXRsT0iJgGnA7MabeTpHHAj4G5EfGGiJgBnATs0LTbtyJiOrAPcIakyU3bLsy3HQ5cJtVlOnEzM7Pi7yFvDDwxwLajgQURMa+xIiIWA4tbd4yIxyXdC2wFPNiy7U5Jq4CJwKNFVdzMzKqpLqOsi2iQx0taBIwja0APGGC/qcAtQylQ0pS8vNvbbNsbWAM8tk61NTOz+hAe1NWk0WW9C3AgcIWGcLki6Xv5/eCrmlYfIel24F7gHyPi+aZtp+QN/wXAERERLeXNkrRQ0sLHVjw+8rMyMzProELvw0bEArKu5EmSzskHYS3KNy8B9mza9zDgA8BmTUV8KyJ2B2YC50nasmnbhXnD/9aIuL5N7LkR0R8R/ZMmbl7kaZmZWQ+TVOjSLYU2yJJ2AcYAj0fEGXkDOj3ffCWwT8sI6de0KyciFgLfAD5aZP3MzMx6VZH3kCHrzT82Il71JvqIWCnpYODzki4Cfg88DXxmgHL/AbhF0rkF1NHMzOqqJveQR9wgR8SYYex7F/CnA2z7GvC1ps8PA40u69nrXEEzM6uvLj87XCQ/y2tmZtYDPJe1mZlVmmrSZe0M2czMrAc4QzYzs2qryT1kN8hmZlZdnqnLzMzMiuQM2czMKs0vlzC03ga1iAEQq14qP8iYzvzvFmteNS9N4d6xbGnpMQC+P3mXjsQ5bPlvSo/RMv18aeryxxk68+8/Vq8qPQZ05ndfdW6QzcyswuR7yGZmZlYcZ8hmZlZtNblN4QbZzMyqS9SmQXaXtZmZWQ9whmxmZtXmDNnMzMyK4gzZzMwqTNBXj9yy8LOQ9Mwg23aS9CNJv5V0s6TrJL0t3/YBSY9JWiTpLkmnNB03W9JD+bbFkg4put5mZlZRUrFLl3TsskLSOODHwNyIeENEzABOAnZo2u1bETEd2Ac4Q9Lkpm0X5tsOBy6TVI9LIjMzMzrbZX00sCAi5jVWRMRiYHHrjhHxuKR7ga2AB1u23SlpFTAReLTcKpuZWU/zY0/rZCpwy1B2lDQFGAfc3mbb3sAa4LFCa2dmZtZFXRvUJel7wE7A3RHxF/nqI/J7yrsAJ0bE802HnCLp/cDTwBHRMlO9pFnALIApk5t7us3MrNacIQ9O0jn5IKxF+aolwJ6N7RFxGPABYLOmw74VEbsDM4HzJG3ZtO3CiJgeEW+NiOtb40XE3Ijoj4j+SRM3L/x8zMysF+WjrItcuqS0yBFxRt6ATs9XXQns0zJC+jUDHLsQ+Abw0bLqZ2Zm1ks61mUdESslHQx8XtJFwO/Jup8/M8Ah/wDcIuncTtXRzMwqqCZd1oU3yBExYZBtdwF/OsC2rwFfa/r8MNDosp5dWAXNzMx6kGfqMjOz6vJjT2ZmZlYkZ8hmZlZtNcmQ3SCbmVmF+eUSZmZmViBnyGZmVm016bJ2hmxmZtYDnCGbmVl11eixJzfIIxAvPFd6jE79b6ax65UeI1a9WHoMAI1dv/QY8cQjpccAOGz5bzoS5/F3zCw9xmY/fdUU9KWI58v/d8m4Aec/KtZLz699n5HqwL+X0tWkQXaXtZmZWQ9whmxmZpUlhPzYk5mZmRXFGbKZmVVbTe4hu0E2M7PqqtEoa3dZm5mZ9QBnyGZmVm3OkM3MzKwoQ2qQJa2WtEjSbZJukTTgLAKSjpG0WNIdkm6VdGq+XpLOlHSPpLslXSdpatNxy/Jjbpf0C0nbtom/WNK3Jb1mJCdtZmZ1kb/tqcilS4YaeWVETI+IacDpwJx2O0k6CDgZeHdE7Aa8GXgy33wCMBOYFhE752XMkzSuqYj9I2J3YD5wZpv4uwIvAscPsd5mZmaVsC6XAhsDTwyw7XTg1Ih4GCAiXoiIr+bbTgNOjIjn8m0/AW4Ejm5TzgJg6wFiXA/suA71NjOzOpKKXYYUUgdK+o2keyV9cpD93ispJPWvrcyhDuoaL2kRMA7YCjhggP12BW5uU6GNgQ0j4r6WTQuBqa37AwcC329TzljgIOA/hlhvMzOrsy489iRpDHAx8C5gOXCTpHkRsbRlv42AjwK/Hkq5w+2y3oWssbxCKuUncJ2kh8ga3W82rW9cECwEHgAubT1Q0ixJCyUtfGzF4yVUzczMDLJUWFMAAB1KSURBVIC9gHsj4r6IeBH4V+DQNvudDfwDMKS3hAy7yzoiFgATgUmSzskHWy3KNy8BZrQ55ingWUk7tGyakR/TsD+wLbAISJvWNy4IpkfESfkPoDXG3Ijoj4j+SRM3H+5pmZlZVXW+y3pr4MGmz8tpuc0qaU9gckT8eKinMewGWdIuwBjg8Yg4o9FQ5pvnAOdL2jLfd31JH863nQ98UdL4fNs7gX2BK5vLj4hVZAPDjpG02XDrZ2ZmNkITGz2u+TJrOAdL6gM+D3xsOMcN9x4yZD32x0bE6tadIuJqSa8Drs27tAO4LN/8JWBT4A5Jq4FHgEMjYmWbcv5b0jfJRmafPZwTMjOz0URlPKq0IiIGG4T1EDC56fM2+bqGjcjGVM3P7+5uSfZU0SERsXCgQofUIEfEmKHsl+97OXB5m/VB1g2dvuqgbPt2LZ9Pavq+Q28DNzOzyun8TF03ATtJ2p6sIT4SeF9jY0Q8SXZrN6+e5pM9gTRgYwyeqcvMzGxY8lurJwLXAHcC/xYRSySdJemQdS3Xc1mbmVl1deltTxFxNXB1y7pPD7DvfkMp0xmymZlZD3CGbGZmFVbKoK6ucINsZmbV5tcvmpmZWVGcIZuZWbXVJEN2gzwSLw5pelJrUGc6ZOKlF0qPoU23LD0GwJrfL+tInM1+8svSYzx2wL6lxwCYdN2N5QeJKD8GoPXHlx4j1rxqjifrEjfIZmZWXV167KkMvodsZmbWA5whm5lZhfmxJzMzs97gLmszMzMrijNkMzOrNmfIZmZmVhRnyGZmVl2iY3MclG2dzkLSM4NsO0bSYkl3SLpV0qn5ekk6U9I9ku6WdJ2kqU3HLcuPuV3SLyRt27RttaRFebnflvSadam3mZnVjaCv4KVLCr2skHQQcDLw7ojYDXgz8GS++QRgJjAtInYG5gDzJI1rKmL/iNgdmA+c2bR+ZURMj4hdgReB44ust5mZWbcVneefDpwaEQ8DRMQLEfHVfNtpwIkR8Vy+7SfAjcDRbcpZAGw9QIzrgR0LrbWZmVWX+opduqToyLsCN7eulLQxsGFE3NeyaSEwtXV/4EDg+23KGQscBNwx8qqamZn1jl4b1HWdpM2AZ4C/b1o/XtKi/PvrgUtbD5Q0C5gFMGXy5LLraWZmvcKPPYGkc/LBVo3Gcgkwo3W/iHgKeFbSDi2bZuTHNOwPbAssAtKm9Y17yNMj4qSIeLFNjLkR0R8R/ZMmbj6S0zIzs6pQPnVmkUuXjChyRJzRaCjzVXOA8yVtCSBpfUkfzredD3xR0vh82zuBfYErW8pcRTYw7Jg8WzYzM6u9QrusI+JqSa8DrpUkIIDL8s1fAjYF7pC0GngEODQiVrYp578lfZNsZPbZRdbRzMxqpiZd1uvUIEfEhEG2XQ5c3mZ9kHVDp686KNu+Xcvnk4YSz8zMrA56bVCXmZnZ8IzmmbrMzMysWM6Qzcys2kbzPWQzM7Oe0HjsqQbqcRZmZmYV5wzZzMyqrSZd1s6QzczMeoAzZDMzq7aaPPbkBnkkXnjVJGOFi9WrSo8BoDHl/6/QiRgAdCpOB/S9bruOxIlVr5oevnBbzF9QegyA387oLz3GDv95XekxAKIDDY3GbVh6DCixS1mCPndZm5mZWUHqk0qYmdnoVJMu63qchZmZWcU5QzYzs2qryWNPbpDNzKzC5C5rMzMzK44zZDMzqy7hx57MzMysOGttkCWtlrRI0m2SbpE0c4D9Zkt6KN93saRD2qxfKumopmMk6UxJ90i6W9J1kqY2bV8m6Y58WSrpM5LGFXHiZmZWE1KxS5cMJUNeGRHTI2IacDowZ5B9L4yI6cDhwGXSH+60N9YfClwiab18/QnATGBaROyclz2vpdHdPyJ2A/YCdgAuGerJmZnZKKC+YpcuGW7kjYEn1rZTRNwJrAImtqy/B3gO2DRfdRpwYkQ8l2//CXAjcHSbMp8BjgfeI2mzYdbbzMyspw1lUNd4SYuAccBWwAFrO0DS3sAa4LGW9XsC90TEo5I2BjaMiPtaDl8ITKWNiHhK0v3ATsCvh1B3MzOrsxrNZT2UBnll3t2MpLcAV0jaNSKizb6nSHo/8DRwRESEsv74UyQdB+wM/PkI69z2Jy9pFjALYMrkySMMYWZm1lnD6rKOiAVk3dCTJJ2TD9Ra1LTLhfn95rdGxPUt66cC7wUulTQuIp4CnpW0Q0uYGcCSdvElbQRsB9zdpm5zI6I/IvonTdx8OKdlZmZVNhrvIUvaBRgDPB4RZ+SN7/ShHh8R88i6pI/NV50PfFHS+Lz8dwL7Ale2iT0B+Efg+xGx1vvYZmZmVTKce8iQdRcfGxGrRxDzLOBKSV8FvkQ2wOsOSauBR4BDI6L5RcPXKev37gO+B5w9gthmZlY3o2Uu64gYM5SCImL2UNZHxM3AHzWtSvOl3bHbDSW2mZmNVp7L2szMzArkuazNzKy6PJe1mZmZFckZspmZVVtN7iG7QTYzs2qrySjrelxWmJmZVZwzZDMzqzBBXz1ySzfII6DNXld+jDH+FVn5NHb9blehMDv84trSYyx405tLjwEw8/62swgXKp57qvQYrBnJXFKjh//am5lZdYna3EN2g2xmZtVWk1HW9TgLMzOzinOGbGZmFabadFk7QzYzM+sBzpDNzKzaavLYUz3OwszMrOKcIZuZWXXV6LGnYWfIkp4ZYP1sSQ9JWiRpsaRD2qxfKumopmMk6UxJ90i6W9J1kqY2bV8m6Y58WSrpM5LGrcuJmplZHSl77KnIpUuKjnxhREwHDgcuk/5wZo31hwKXSFovX38CMBOYFhE7A3OAeS2N7v4RsRuwF7ADcEnBdTYzM+u6Ui4FIuJOYBUwsWX9PcBzwKb5qtOAEyPiuXz7T4AbgaPblPkMcDzwHkmblVFvMzOrIKnYpUtKaZAl7Q2sAR5rWb8ncE9EPCppY2DDiLiv5fCFwFTaiIingPuBnYqvtZmZWfcUPajrFEnvB54GjoiIUHa1cYqk44CdgT8fYYy2ly+SZgGzAKZMnjzCEGZmVhmjfepMSefkA7UWNa2+MCKmR8RbI+L6lvVTgfcCl0oal2e7z0raoaXoGUDbV5xI2gjYDri7dVtEzI2I/ojonzRx83U9LTMzqxIJ+gpeumSdG+SIOCNvfKcP45h5ZF3Sx+arzge+KGk8gKR3AvsCV7YeK2kC8I/A9yPiiXWtt5mZWS/qxnPIZwFXSvoq8CWyAV53SFoNPAIcGhErm/a/Tlm/dx/wPeDsTlfYzMx6WE26rIfdIEfEhAHWzx7K+oi4GfijplVpvrQ7drvh1s/MzKyKPFOXmZlVW01m6nKDbGZmFabadFnX4yzMzMwqzhmymZlVmmrSZe0M2czMrAc4QzYzs+oSvodsZmY2Wkk6UNJvJN0r6ZNttv9d/trg2yX9TNK2ayvTDbKZmVVY59+HLGkMcDFwEPAm4ChJb2rZ7VagPyJ2B74DfHZt5brLegTW3LWw9Bh9MzYpPQYAG2xYegj11ef6L9as6XYVClWn340mlP9vZub9bafbL9zT731n6TEmXPqqmYqLF1Fu+Z2ff3ov4N7G2wol/StwKLC0sUNEXNe0/6+A96+t0Pr8KzQzMyvGREkLm5ZZLdu3Bh5s+rw8XzeQDwH/vragzpDNzKzaih/UtSIi+osoKH8lcT/w9rXt6wbZzMxseB4CJjd93iZf9wr5GwzPAN4eES+srVA3yGZmVl2iG3NZ3wTsJGl7sob4SOB9r6iWtAdwCXBgRDw6lELdIJuZWYV1fi7riFgl6UTgGmAMcFlELJF0FrAwIuYB5wMTgG/nM4k9EBGHDFauG2QzM7Nhioirgatb1n266fthD5F3g2xmZtXmuazNzMysKIU3yJJWS1ok6TZJt0iaOci+x0haLOkOSbdKOlXSNEmLmvY5StJKSevln3eTdHvR9TYzs4rq8ExdZSmjy3plREwHkPQnwBzaPH8l6SDgZODdEfGwpA2AY4A7gCmSNoqIp4GZwJ3AHsB/5Z9vLKHeZmZWNVI3ZuoqRdmXAhsDTwyw7XTg1Ih4GCAiXoiIr0bEGmAhsHe+3wyyOUMbmfZM4IbyqmxmZtZ5ZTTI4/Mu67uAfwbOHmC/XYGbB9h2AzBT0obAGmA+r2yQnSGbmVmmJl3WZUReGRHTI2IX4EDgCmnYQ+BuJGt49wJuiojfAjtKmgRMyD+/gqRZjXlHH1vx+EjPwczMrKNKvRSIiAXARGCSpHPyzLkxYGsJWXd0O78C/hjYB1iQr1tONhvKgnYHRMTciOiPiP5JEzcv7BzMzKzHScUuXVJqgyxpF7JZTB6PiDPyzHl6vnkOcL6kLfN915f0YYB8MNeDwHG83AAvIBsE5vvHZmZWO2WMsh7flAULODYiVrfuFBFXS3odcG3epR3AZU273AAcGhGNV1wtAM7F94/NzOwPOj91ZlkKb5AjYsww9r0cuHyAbScAJzR9nk/WwJuZmb3MM3WZmZlZUTyXtZmZVZeoTZd1Pc7CzMys4pwhm5lZhQn66pFbukE2M7NKG/7cU72pHpcVZmZmFecM2czMqq0mg7rcII/E5luWH2ODDcuPAfDCs+XHGL9R+TE6JjoTZvWqzsTp26AzcWxYJnzr6tJj/Havt5Qe44Vly0uPUQdukM3MrLpEbSYGcYNsZmYVVp+pM+txFmZmZhXnDNnMzKqtJl3WzpDNzMx6gDNkMzOrtprM1FWPszAzM6s4Z8hmZlZdku8hD5WkZwbZdoykxZLukHSrpFMlTZO0qGmfoyStlLRe/nk3SbeXXW8zM6sI9RW7dEnXIks6CDgZeHdE7Aa8GXgSuAOYIqkxrdNM4E5gj6bPN3a4umZmZqXq5j3k04FTI+JhgIh4ISK+GhFrgIXA3vl+M4CLyRpi8q83dLqyZmbWoxrd1kUtXdLNBnlX4OYBtt0AzJS0IbAGmM8rG2RnyGZmViu9Osr6RrKGdy/gpoj4LbCjpEnAhPzzK0iaJWmhpIWPrXi8w9U1M7PuUcFLd3SsQZZ0jqRFTQO2lpB1R7fzK+CPgX2ABfm65cCRTZ9fISLmRkR/RPRPmrh5gTU3M7PeVXB39Wjoso6IMyJiekRMz1fNAc6XtCWApPUlfTjf92ngQeA4Xm6AF5ANAvP9YzMzq52udVlHxNXAl4FrJS0BbgE2btrlBmCDiHgw/7wA2AHfPzYzs2Y1yZBLnxgkIiYMsu1y4PIBtp0AnND0eT7d7Nw3MzMrkWfqMjOziqtHruYG2czMqkt46kwzMzMrjjNkMzOrtnokyM6QzczMeoEzZDMzq7h6pMjOkM3MzHqAM2QzM6uw7k7mUaRaNsg337pohTbc5HfDPGwisKKM+nQ4Rqfi+Fx6M47PpTfjjPZz2baMivyBG+TeFRGThnuMpIUR0V9GfToZo1NxfC69Gcfn0ptxfC42FLVskM3MbDSpR4bsQV1mZmY9wBnyy+bWJEan4vhcejOOz6U34/hcylSTe8iKiG7XwczMbJ30T9stbvrJvELL7Ntyh5u7cZ/cXdZmZmY9wF3WZmZWXfJzyDZEksYBO+Yf742I57tZHzMz602jukGWtBuwS/7xzohYXGDZY4FzgQ8CvyMblz9Z0uXAGRHxUlGxBqnDlIh4oCoxJL2fbFzDN1rW/zWwOiKurEKMlnInRkSpEzWUHSO/qDye7MLyDuDSiFhVtRgt8TYBdso/3h0RT1YthqTbgBvy5caIuL/I8jsVoxA1yZBH5T1kSa+VNB/4PvA+4GjgB5Kuk7RxQWHOBzYDto+IGRGxJ/AGYBPggoJiACDpLZL+UtIW+efdJV1J9o+oMjGAk4DvtVl/FfCxCsVA0p9Legy4Q9JySTOLKruTMXJfB/rJGsqDgM9VNAaSNpD0NWAZ2WjhrwLLJF0maf2qxMgdDSwC3gVcI+khSd+RdIqkvSsUowAqeOmOUTnKWtIXgReBT0TEmnxdH3AeMD4iTiogxj3AztHyA5Y0BrgrInZqf+Sw45wPHEz2j2ZH4Brgw8Ac4JIiusg7ESOPc0t+4dJu2+0RsXsVYjTKAv4qIu7K/3B9NiLeXkTZnYyRx7kjInbLvx8L/NdAP8NejpGXfRbZhfHxEfF0vm4j4GLgdxHx91WIMUDcicCRwMlkicCYKsYYrv7pu8dNP/1RoWX2bbFtV0ZZj9Yu63cCuzcaY4CIWCPpU2RX6EWI1sY4X7laUpFXQX8G7BERz0vaFHgQ2DUillUsBsB4SRtGxLPNK/M/ZkVlFp2IAbAqIu4CiIhf5+UXrRMxAP5weyUiVqmc7sFOxAD4C2CviHiuKd7Tkv4v8CugiMayEzEaF/d7ADOBfcguAh4C/hlYUJUYRSjx/5eOGq0N8ovt7k/lfwheKCjGUknHRMQVzSvze5h3FRQD4PlGhhoRT0i6p4SGshMxAC4FviPp+Ij4HYCk7cgyi0srFANgC0l/N9DniPh8RWIATJP0VP69yC5qnsq/j4go4jZPJ2IArGluKBsi4pkCL5Q7EQPgaWAp2f+7nyzp/m4nYlhutDbI4yTtwatvFgjYoKAYJwBXSfogcHO+rh8YDxxWUAyAHSQ1PxW/ffPniDikIjGIiAskPQP8UtIEst/H08B5EfFPVYmR+yqw0SCfqxKDTnRLdrDrM/JennYp1Zo263o1BsCHgLeQ3T46TtJNZFnrgoh4qEIxRq4mGfJovYd83WDbI2L/AmMdAEzNPy6NiJ8VVXZe/qD3DCPiF1WI0SbmRnnZTxdddidjDBD3VV3mFY2xCXBCRJxTlRiSlpE1iu3+gkdE7FCFGG1ivgbYi6xr+Thg/Ygo9JWHnYixLvqn7x4Lf3Z1oWVq4mTfQ+6UwRpcSesVHOvnwM+LLLPFrRHxVLsNkqZUKEajvF2Bj5NfxEhaAlwQEUXd2+9IjLzcrYGtgNsj4kVlI9RPBj4AvL5CMSaT3fd8PdmTCd8EzgKOAQp5TKwTMQAiYruiyupmjAZJGwJ78/I93j8mG+NR5BMWpccYme6OjC7SqHzsqZUy75B0KbC82/UZpvmNbyS1Zt/fr1AMJB1K9kjSL8ie3/5g/v1V+bZKxMjjnEw2Kv1LwK8kfRi4k+yWxYyqxMhdATycx5kKLCRrOHeLiI9WKAYAktaXdJykC/LlOElF3arqZIxbyeY4+ATZ3/LPAdtFxB4RcWJVYhSiMVtXUUu3TmM0dlk3SHoz2XPI7yF7ZvgEYF5EPNHVig2DpFsjYo/W79t97uUYeVm3AYe2DhjLB139ICKmVSFGXt5SYN+I+J+8F+FuYJ+IuHkth/ZUjDzObc0/F0nLgSnNTylUIUZe7puAeWTZXePnNIMs8zs0IpZUIUYeZ3fgjnZPcxSlEzFGqn/6tFj4838vtExtvrW7rDtF0rnA4cADZF1jKbAwIr7e1Yqtmxjg+3afezkGwNh2o7cjYlmBtxI6EQOyken/k5f9gKTfFN1QdigGAC2DlB4HXitlqUSjDlWIQZaB/01E/LQl9juBLwNFjB/pRAzIHt/8Y1qeDpD0IWCjiLioIjFGRtRmUNeobJDJRgzeDfwT8MOIeKHgxxE6qfGoi3jlYy8CJlUoBsAqtZmKU9K2QFHTKHYiBsA2yiagadiq+XNE/G1FYgC8lizTa/6rd0sjDFDEIKVOxADYurWhBIiIayV9qUIxIJtF681t1n+DrMu/iMayEzEsN1ob5K3IpoI7CrgoH3U9XtLYds8n97jmR11aH3v55wrFAEiAa/MejOZHxT6ZL1WJAdmgsWZlZK6diFG3gVB9kjaIiFfMN6BsLu2i/h52IgZkvT2vmhM/H9xXVMrYiRgF6KGqjMCobJAjYjXwH8B/5AMtDiYbCPOQpJ9FxPu6WsFhiIgUQCW+YKATMfI435d0P9mc0o3pS5eQTQ95W1Vi5HFKv/3RyVssyuZgPpqXH+FbAlzZ2uj0egyywWPflXRCvHJimC+SZX1ViQFZw/+6iPh980pJr6tYjJHrpWuDERiVo6wljZN0sqQvA8eSDeb5S7I3s/xHd2s3PJIOVvaCgdtV3ksMSo/REBG3RcQxkb2QY0b+/W2SCntrVSdiSNpX0jFNn78j6ef5ckBVYuTlvolstqb9yMZdPJB/v0TS1IGP7K0YABHxGbJ/49dLWiFpBdko+59GxFlViZE7H/ixpLdL2ihf9gN+RHEvsOlEDMuNylHWkr5FNnfu9WRvllkWESd3t1brRjV6icFa6vBgREyuSgxlj4edFBFL8893kD0bvCHwqYg4sAoxmuKcN8AgpTMGe66/l2K0iVn5yWckHUR2q2VXsnvtS8h+joUNO+5EjJHo32NaLJz/k0LL1CZbepR1B70pXn6zzKXAf3W5PiNRp5cYDKYTV45Fxti40VDm7mmMgJY0p0IxoF4DoVD2woRNG7df8q7yDwCnRMQbqxIDIG8US20YOxHDMqO1Qe7Um2U6oTYvMWiJ8YpNwISqxMht0vwhIv6i6WNR9986EQNqNBBK0pHAJcCzyl6Reg5wGXAT2f3rSsQYJPaArxetUozhqc9MXaO1Qe7Um2U6oTYvMVhLmV+oUAyAuyT9WUT8uHmlpIOB31QoBtRrINSZwIyIuFfSnmQvSvjLiPhhxWIMpBMtU++1ftVOqv5gVDbI0QMv1S5KYwR01WN0Kk6nzgU4hWwwzF/y8vO0M8jmAz64QjGIiM9IOpFskNJr8tXPks3/XUh3cidi5F6MiHvzmLcoe5Vo0Q1lJ2IM5Mdr36USMUalUTmoq+7q1G1V5XPJH6lr9xjP81WK0RKv0gOhlE3J2XyL5e+aPxdx+6UTMQaJPRF4PEbRH/b+PafHwl9cW2iZ2niSB3VZYerUbVXZc8nvh15WRtmdjJHfd38yIi5tbiRV4PSJnYiRq80tHmVz8Z8H/A9wNlnX/kSy+/HHRMSIH+GU9DTtBztW8fZez3ODXE916raqzblUONuvzRSNUaOJdMjmxf4U2bSjPwcOiohfSdqFbI7+ETfIEdGNJyrWQT3uIY/KiUFGgYtU8tDxiDizzPKb1OlcqprtDzh9YoHxOhGjbhPpjI2In0TEt4FHIuJXAI1HFK163CBXnKQ3S5ov6SpJe0haDCwGfi+pqIkhnpb0VJvl6abR6kXEqc25DKCq2X6f2kyV2G5dj8cAOBd4a0S8HngvUOTz2p2MAdD8asqVLdtGzT1kgEHfbVyh9yG7y7r66tRtVadzaeciSSpzwE1J2X5j+sSP8crR3OdT/BSNZcaAek2k03h8s/nRTfLP40qK2YO624gWyQ1y9Y2NiJ8ASDqruduqghOe1OZc6jTgJiKuyLtgz+KV0yd+uqjpEzsRI1ebiXTq9PhmFeW9dl8AxgD/HBHntWzfgOz5+hlk7/c+Itq8i72ZG+Tqq1O3VZ3OpVbZfo2maKzNKGtr1tkLdmVTo15M9hrf5cBNkua1TGX7IeCJiNgxn73tH4AjBivXDXL11anbqk7nUptsv52qjhiv2eQz1j17AfdGxH0Akv4VOJTsjWUNhwKz8++/A3x5bbes3CBXXJ26rep0LtQr22+nqiPGXx2kohcX1qTzF7lbAw82fV4O7D3QPvk7E54ENgcGfBTODbJZOeqU7bdT1RHj7dTm4mI0uvnWRddow00mFlzsOEkLmz7PjYi5Bcd4FTfIZiWoWbbfTlVHjLdTp4uLUScKeu/3MD0ENL87fZt8Xbt9lksaSzae5PHBCvVzyGY2qFHwfHidJp+xzrgJ2EnS9sredX0kMK9ln3nAsfn3fwn8fG0XsH65hJkNKu+6a4wYn0vLiPGI2KOrFRyGwR5HAyr1OJp1l6Q/JZvSdQxwWUScI+ksYGFEzFP2Lu9vAHuQ/f92ZGMQ2IBlukE2s8FIWhQR0/Pv74yINzZtu7ViDXJtLi6sftxlbWZrU6cR457/2XqWB3WZ2drUacR4nS4urGbcZW1mo4ak1cCz5BcXwHONTcC4iFivW3Uzc4NsZmbWA3wP2czMrAe4QTYzM+sBbpDNzMx6gBtkMzOzHuAG2czMrAf8f2Gab0Vn1BJlAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_matrix(predict, label):\n",
        "                c_matrix = confusion_matrix(label, predict, normalize='true')\n",
        "                plt.figure(figsize=(8,8))\n",
        "                plt.imshow(c_matrix, cmap=\"Reds\")\n",
        "                plt.title('Confusion matrix', fontsize=20, color = 'grey', pad=16)\n",
        "                plt.xticks(np.arange( 14),labels= ['PAD', 'O', 'B-PER' ,'I-PER' ,'B-LOC' ,'I-LOC' ,'B-GRP' ,'I-GRP' ,'B-CORP' ,'I-CORP' ,'B-PROD' ,'I-PROD' ,'B-CW' ,'I-CW'], rotation = 90 )\n",
        "                plt.yticks(np.arange( 14),labels= ['PAD', 'O', 'B-PER' ,'I-PER' ,'B-LOC' ,'I-LOC' ,'B-GRP' ,'I-GRP' ,'B-CORP' ,'I-CORP' ,'B-PROD' ,'I-PROD' ,'B-CW' ,'I-CW'] )\n",
        "                plt.colorbar()\n",
        "\n",
        "                plt.show()\n"
      ],
      "metadata": {
        "id": "fZ4pKgVLXLlM"
      },
      "execution_count": 39,
      "outputs": []
    }
  ]
}